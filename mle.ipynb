{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdaa8bc-165d-4564-a313-39f177062f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from numpy.random import normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee5edc-d1a5-465b-9422-56a289b4500a",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a0607-bb9c-4c57-9c97-7f879bd223c0",
   "metadata": {},
   "source": [
    "### Likelihood Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c2ddc-1288-42d5-8037-b4231138a434",
   "metadata": {},
   "source": [
    "To understand how to fit a distribution from data points observed, we need to have some assumption about the underlying distribution. For example, given the height (or weight) of 100 individuals, one might want to fit a multivariate (perhaps correlated) gaussian to the data set.\n",
    "\n",
    "How do we accomplish this? Let's cover some basic definitions\n",
    "\n",
    "> **Definition**: Likelihood\n",
    "Likelihood refers to the **value** of the probability density for a certain value of the observed random variable.\n",
    "\n",
    "> **Definition**: Point Estimation is the process of finding the best fitting parameter of a distribution, given the data observed.\n",
    "\n",
    "Let $f(\\mu, \\sigma; x_i)$ be the value of the probability density at point $x$, given the mean $\\mu$, and standard deviation $\\sigma$. Assume $X \\sim N(\\mu, \\sigma)$, therefore, we know that the likelihood is:\n",
    "$$f(\\mu, \\sigma; x_i) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\left( -\\frac{(x_i - \\mu)^2}{2 \\sigma^2}\\right)}$$\n",
    "for datapoint $x_i$.\n",
    "\n",
    "If we have $n$ datapoints: $x_1, x_2, \\dots x_n$, then the joint probability of these independent observations is:\n",
    "$$L(\\mu, \\sigma; \\vec{x}) \\equiv f(\\mu, \\sigma; x_1, x_2, \\dots , x_n) = \\prod_{i=1}^n  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\left( -\\frac{(x_i - \\mu)^2}{2 \\sigma^2}\\right)}$$\n",
    "\n",
    "We call this the likelihood function $L$\n",
    "\n",
    "How do we maximize this $L$ with respect to parameters $\\mu, \\sigma$? \n",
    "\n",
    "We can take a derivative and find the maximum. In reality, taking a derivative of the **log likelihood** is easier since a derivative of a sum is easier than a derivative of a product!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b1aec-b57d-477f-8f43-684710a00241",
   "metadata": {},
   "source": [
    "### Log Likelihood Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e435c9a-4916-4e93-a5ac-3c46b289a97e",
   "metadata": {},
   "source": [
    "We can solve the following system for $\\mu, \\sigma$: \n",
    "\n",
    "$$\\ln{L(\\mu, \\sigma; \\vec{x})} = \\sum_{i=1}^n -\\ln{\\sigma} -\\frac{(x_i - \\mu)^2}{2 \\sigma^2}$$\n",
    "$$\\frac{\\partial \\ln L}{\\partial \\mu} = 0$$\n",
    "$$\\frac{\\partial \\ln L}{\\partial \\sigma} = 0$$\n",
    "\n",
    "Or equivalently,\n",
    "> $$\\arg \\max_{\\mu, \\sigma} \\ln L$$\n",
    "\n",
    "The analytical solution as one might expect is:\n",
    "> $$\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i$$\n",
    "$$\\hat{\\sigma} = \\frac{1}{n} \\sum_{i=1}^n (x-\\hat{\\mu})^2$$\n",
    "\n",
    "\n",
    "Let's try to do this numerically to learn something new:\n",
    "\n",
    "- we first generate a sample dataset $x$ sampled from a normal distribution $N(0, 20)$\n",
    "- then we maximuize the log likelihood by minimizing -1 * LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f733c098-128f-481f-a651-7973056c25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = normal(0, 20, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a142aa2-f383-433a-93c8-fbb2a74d9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(p):\n",
    "    mu, sigma = p[0], p[1]\n",
    "    ll = (-np.log(sigma) - (x - mu)**2 / (2 * sigma**2)).sum()\n",
    "    return ll * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e714b6-677f-4bec-a33b-7788d4726793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2384a246-254f-4bb8-b3a1-cbe6a77cf941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 3487.0916733110157\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([5.91171556e-04, 9.09494627e-05])\n",
       "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 60\n",
       "      nit: 19\n",
       "     njev: 20\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-0.26202289, 19.8279443 ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(log_likelihood, x0 = [0, 1], method='L-BFGS-B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6b95c-69f6-4e61-9fd9-4f493dd98140",
   "metadata": {},
   "source": [
    "The result indicate a $\\mu \\approx -0.26, \\sigma \\approx 20$, which is quite close to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4898cd-dc6a-4778-a916-1226fb67c2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
