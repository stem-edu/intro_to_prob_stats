{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97335a93-6462-4dff-9eb8-a62fcdeb1e2e",
   "metadata": {},
   "source": [
    "## Discrete probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c14f5-4d66-4283-90e5-88caddc1fedf",
   "metadata": {},
   "source": [
    "### Key terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6e442-6a07-448a-bda2-ff26cc2a561c",
   "metadata": {},
   "source": [
    "First, it is important to define a few terms.\n",
    "\n",
    "Definitions:\n",
    "- **Event** : A physical event that happens due to the probabilistic nature of the world\n",
    "- **Random Variable**: A function that takes probabilistic **events** and transforms them into a number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef59c95-9dad-4d7b-923f-4a144042643c",
   "metadata": {},
   "source": [
    "### Learning random variables with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a79ab8-5eb2-4c36-80c3-5acf912b86e7",
   "metadata": {},
   "source": [
    "<u> Example 1: Fair Coin toss</u> \n",
    "\n",
    "$\\Omega = \\{H, T\\}$\n",
    "$$P(H) = P(T) = \\frac{1}{2}$$\n",
    "\n",
    "In other words, a fair coin toss implies that the probability of getting heads is 0.5, same as that of getting a tail on 1 toss, theoretically. \n",
    "\n",
    "Whenever we encounter a binary outcome that has a certain probability $p$ of occurring (the other outcome occurring with $1-p$ probability), this is known as a **Bernoulli** random variable.\n",
    "\n",
    "Using this example, we can try to understand what expected value means:\n",
    "\n",
    "**Expected value** of a random variable is the average or mean, the expected value is usually written as $E[X]$. \n",
    "\n",
    "If the sample space contains $n$ events:\n",
    "\n",
    "$$\\Omega = \\{ E_1, E_2, \\dots, E_n \\}$$\n",
    "\n",
    "then the expected value is defined as:\n",
    "$$E[X] = P(E_1) E_1 + P(E_2) E_2 + \\dots P(E_n) E_n = \\sum_{i=1}^n P(E_n) E_n $$\n",
    "\n",
    "Additionally, the expectation operation is <u> Linear </u>, meaning if two random variables are *independent*, meaning random variable's occurence does not affect the other's occurence, then:\n",
    "\n",
    "$$E[X_1 + X_2] = E[X_1] + E[X_2]$$\n",
    "\n",
    "In this example:\n",
    "$$E[X] = p \\cdot 1 + (1-p) \\cdot 0 = p$$\n",
    "\n",
    "Another important definition to introduce is the concept of variance, meaning how spread out the random variable is, by definition:\n",
    "\n",
    "$$Var[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2$$\n",
    "\n",
    "We already know how to evaluate $E[X]$, but how do we evaluate $E[X^2]$?\n",
    "\n",
    "$$E[X^2] = \\sum_{i=1}^n P(E_i) E_i^2$$\n",
    "\n",
    "In the context of a bernoulli variable:\n",
    "\n",
    "$$E[X^2] = p \\cdot 1^2 + (1-p) \\cdot 0^2 = p$$\n",
    "$$E[X]^2 = p^2 $$\n",
    "\n",
    "Hence, the variance of a Bernoulli random variable is:\n",
    "$$Var[X] = p - p^2 = p(1-p)$$\n",
    "\n",
    "<u> Example 2: Fair Die Roll</u>\n",
    "\n",
    "$\\Omega = \\{1,2,3,4,5,6\\}$\n",
    "$$P(1) = P(2) = \\dots = P(6) = \\frac{1}{6}$$ \n",
    "\n",
    "Again, each event (or value on the roll) has identical probability. The value of a die roll is also known as a **Discrete uniform** random variable.\n",
    "\n",
    "\n",
    "<u> Example 3: Number of heads in 3 fair coins</u> \n",
    "\n",
    "Since we already discussed that the outcome of a fair coin is a Bernoulli random variable wit $p=\\frac{1}{2}$, in other words $X \\sim \\text{Bern(p)}$.\n",
    "All of the probable events that could happen when you toss 3 fair coins are:\n",
    "$$\\Omega = \\{HHH, TTT, HHT, HTH, THH, HTT, THT, TTH\\}$$\n",
    "\n",
    "If we care about the order of the coin toss, i.e. HHT is different from HTH, then each outcome has a probability of $\\frac{1}{8}$. However, once we assume that the coins are identical and we can't distinguish between which coin produced which head, all that we care about is the *total number of heads*, we have a **Binomial** random variable.\n",
    "\n",
    "Assume $N$ represents the total number of heads from the 3 tosses, and $X_i$ represents the number of head for the $i$th coin, then $$N = X_1 + X_2 + X_3$$\n",
    "Hence, the number sample space containing the number of heads is:\n",
    "\n",
    "$\\Omega = \\{0, 1, 2, 3\\}$\n",
    "\n",
    "$$P(N=n) = \\binom{3}{n} p^n (1-p)^{3-n}$$\n",
    "\n",
    "How then can we calculate the mean value of the number of heads? \n",
    "\n",
    "$$E[N] = E[X_1 + X_2 + X_3] = E[X_1] + E[X_2] + E[X_3] = \\frac{3}{2}$$\n",
    "\n",
    "\n",
    "To calculate the Variance property of a Binomial Variable, we must understand how to evaluate the variance of a sum of random variables. \n",
    "\n",
    "As mentioned before, random variables can be independent or non-independent and hence we must understand what **Covariance** represents.\n",
    "\n",
    "$$Var[X_1+X_2] = Var[X_1] + Var[X_2] + 2 Cov[X_1, X_2]$$\n",
    "\n",
    "If $X_1, X_2$ are independent random variables, then $Cov[X_1, X_2] = 0$\n",
    "\n",
    "Hence, the variance of the number of heads for 3 fair coin tosses:\n",
    "\n",
    "$$Var[X_1 + X_2 + X_3] = 3 Var[X_1] = 3p(1-p) = 3 \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{3}{4}$$\n",
    "\n",
    "<u> Example 4: Random count of discrete events </u>\n",
    "\n",
    "One of the most important discrete random variables is the **Poisson** random variable. \n",
    "\n",
    "The Poisson random variable, $X$,  is used to model/describe the probability of a number of events happening within a fixed time interval, assuming a constant mean arrival rate $\\lambda$ (unchanged by when the last event arrived)\n",
    "\n",
    "$$P(X=n) = \\frac{\\lambda^n e^{-\\lambda}}{n!}$$\n",
    "\n",
    "It is a useful distribution to model number of arrivals of customers, number of server jobs arriving in a given hour, number of neuron spikes during a fixed period of time...\n",
    "\n",
    "To derive the mean and variance, again we can apply the definition of the definition of $E[X]$:\n",
    "\n",
    "$$E[X] = 1 \\cdot  \\frac{\\lambda^1 e^{-\\lambda}}{1!} +  2 \\cdot  \\frac{\\lambda^2 e^{-\\lambda}}{2!} + \\dots =  e^{-\\lambda} \\sum_{n=0}^\\infty n  \\frac{\\lambda^n}{n!} = e^{-\\lambda} \\sum_{n=1}^\\infty \\frac{\\lambda^n}{(n-1)!} = e^{-\\lambda} \\sum_{n'=0}^\\infty \\frac{\\lambda^{(n'+1)}}{(n')!} = e^{-\\lambda} e^{\\lambda} \\lambda = \\lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c3cf9-b71d-43c5-9ff4-fe5bca8c849c",
   "metadata": {},
   "source": [
    "## Important Continuous Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c6048-2520-4813-b4e5-f8a267a259bd",
   "metadata": {},
   "source": [
    "<u>Normal Distribution</u>\n",
    "\n",
    "Perhaps the most important continuous distribution used commonly to describe naturally occurring data is the normal distribution (also known as the Gaussian Distribution).\n",
    "\n",
    "<u>t-distribution</u>\n",
    "\n",
    "<u>Exponential Distribution</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f8788-5593-4162-88fd-60f84cbf96fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
