{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97335a93-6462-4dff-9eb8-a62fcdeb1e2e",
   "metadata": {},
   "source": [
    "## Discrete probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c14f5-4d66-4283-90e5-88caddc1fedf",
   "metadata": {},
   "source": [
    "### Key terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6e442-6a07-448a-bda2-ff26cc2a561c",
   "metadata": {},
   "source": [
    "First, it is important to define a few terms.\n",
    "\n",
    "Definitions:\n",
    "- **Event** : A physical event that happens due to the probabilistic nature of the world\n",
    "- **Sample Space**: The set/collection of all events that could happen\n",
    "- **Probability Measure**: A function that takes probabilistic **events** and transforms them into a real number from 0 to 1\n",
    "\n",
    "An important property of a probability measure $P(\\cdot)$ is that $$P(\\Omega) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef59c95-9dad-4d7b-923f-4a144042643c",
   "metadata": {},
   "source": [
    "### Learning random variables with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a79ab8-5eb2-4c36-80c3-5acf912b86e7",
   "metadata": {},
   "source": [
    "<ins> Example 1: Fair Coin toss</ins> \n",
    "\n",
    "$\\Omega = \\{H, T\\}$\n",
    "$$P(H) = P(T) = \\frac{1}{2}$$\n",
    "\n",
    "In other words, a fair coin toss implies that the probability of getting heads is 0.5, same as that of getting a tail on 1 toss, theoretically. \n",
    "\n",
    "Whenever we encounter a binary outcome that has a certain probability $p$ of occurring (the other outcome occurring with $1-p$ probability), this is known as a **Bernoulli** random variable.\n",
    "\n",
    "Using this example, we can try to understand what expected value means:\n",
    "\n",
    "**Expected value** of a random variable is the average or mean, the expected value is usually written as $E[X]$. \n",
    "\n",
    "If the sample space contains $n$ events:\n",
    "\n",
    "$$\\Omega = \\{ E_1, E_2, \\dots, E_n \\}$$\n",
    "\n",
    "then the expected value is defined as:\n",
    "$$E[X] = P(E_1) E_1 + P(E_2) E_2 + \\dots P(E_n) E_n = \\sum_{i=1}^n P(E_n) E_n $$\n",
    "\n",
    "Additionally, the expectation operation is <ins> Linear </ins>, meaning if two random variables are *independent*, meaning random variable's occurence does not affect the other's occurence, then:\n",
    "\n",
    "$$E[X_1 + X_2] = E[X_1] + E[X_2]$$\n",
    "\n",
    "In this example:\n",
    "$$E[X] = p \\cdot 1 + (1-p) \\cdot 0 = p$$\n",
    "\n",
    "Another important definition to introduce is the concept of variance, meaning how spread out the random variable is, by definition:\n",
    "\n",
    "$$Var[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2$$\n",
    "\n",
    "We already know how to evaluate $E[X]$, but how do we evaluate $E[X^2]$?\n",
    "\n",
    "$$E[X^2] = \\sum_{i=1}^n P(E_i) E_i^2$$\n",
    "\n",
    "In the context of a bernoulli variable:\n",
    "\n",
    "$$E[X^2] = p \\cdot 1^2 + (1-p) \\cdot 0^2 = p$$\n",
    "$$E[X]^2 = p^2 $$\n",
    "\n",
    "Hence, the variance of a Bernoulli random variable is:\n",
    "$$Var[X] = p - p^2 = p(1-p)$$\n",
    "\n",
    "<ins> Example 2: Fair Die Roll</ins>\n",
    "\n",
    "$\\Omega = \\{1,2,3,4,5,6\\}$\n",
    "$$P(1) = P(2) = \\dots = P(6) = \\frac{1}{6}$$ \n",
    "\n",
    "Again, each event (or value on the roll) has identical probability. The value of a die roll is also known as a **Discrete uniform** random variable.\n",
    "\n",
    "\n",
    "<ins> Example 3: Number of heads in 3 fair coins</ins> \n",
    "\n",
    "Since we already discussed that the outcome of a fair coin is a Bernoulli random variable wit $p=\\frac{1}{2}$, in other words $X \\sim \\text{Bern(p)}$.\n",
    "All of the probable events that could happen when you toss 3 fair coins are:\n",
    "$$\\Omega = \\{HHH, TTT, HHT, HTH, THH, HTT, THT, TTH\\}$$\n",
    "\n",
    "If we care about the order of the coin toss, i.e. HHT is different from HTH, then each outcome has a probability of $\\frac{1}{8}$. However, once we assume that the coins are identical and we can't distinguish between which coin produced which head, all that we care about is the *total number of heads*, we have a **Binomial** random variable.\n",
    "\n",
    "Assume $N$ represents the total number of heads from the 3 tosses, and $X_i$ represents the number of head for the $i$th coin, then $$N = X_1 + X_2 + X_3$$\n",
    "Hence, the number sample space containing the number of heads is:\n",
    "\n",
    "$\\Omega = \\{0, 1, 2, 3\\}$\n",
    "\n",
    "$$P(N=n) = \\binom{3}{n} p^n (1-p)^{3-n}$$\n",
    "\n",
    "How then can we calculate the mean value of the number of heads? \n",
    "\n",
    "$$E[N] = E[X_1 + X_2 + X_3] = E[X_1] + E[X_2] + E[X_3] = \\frac{3}{2}$$\n",
    "\n",
    "\n",
    "To calculate the Variance property of a Binomial Variable, we must understand how to evaluate the variance of a sum of random variables. \n",
    "\n",
    "As mentioned before, random variables can be independent or non-independent and hence we must understand what **Covariance** represents.\n",
    "\n",
    "$$Var[X_1+X_2] = Var[X_1] + Var[X_2] + 2 Cov[X_1, X_2]$$\n",
    "\n",
    "If $X_1, X_2$ are independent random variables, then $Cov[X_1, X_2] = 0$\n",
    "\n",
    "Hence, the variance of the number of heads for 3 fair coin tosses:\n",
    "\n",
    "$$Var[X_1 + X_2 + X_3] = 3 Var[X_1] = 3p(1-p) = 3 \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{3}{4}$$\n",
    "\n",
    "<ins> Example 4: Random count of discrete events </ins>\n",
    "\n",
    "One of the most important discrete random variables is the **Poisson** random variable. \n",
    "\n",
    "The Poisson random variable, $X$,  is used to model/describe the probability of a number of events happening within a fixed time interval, assuming a constant mean arrival rate $\\lambda$ (unchanged by when the last event arrived)\n",
    "\n",
    "$$P(X=n) = \\frac{\\lambda^n e^{-\\lambda}}{n!}$$\n",
    "\n",
    "It is a useful distribution to model number of arrivals of customers, number of server jobs arriving in a given hour, number of neuron spikes during a fixed period of time...\n",
    "\n",
    "To derive the mean and variance, again we can apply the definition of the definition of $E[X]$:\n",
    "\n",
    "$$E[X] = 1 \\cdot  \\frac{\\lambda^1 e^{-\\lambda}}{1!} +  2 \\cdot  \\frac{\\lambda^2 e^{-\\lambda}}{2!} + \\dots =  e^{-\\lambda} \\sum_{n=0}^\\infty n  \\frac{\\lambda^n}{n!} = e^{-\\lambda} \\sum_{n=1}^\\infty \\frac{\\lambda^n}{(n-1)!} = e^{-\\lambda} \\sum_{n'=0}^\\infty \\frac{\\lambda^{(n'+1)}}{(n')!} = e^{-\\lambda} e^{\\lambda} \\lambda = \\lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c3cf9-b71d-43c5-9ff4-fe5bca8c849c",
   "metadata": {},
   "source": [
    "## Important Continuous Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d07ae2-7120-4522-a167-f5bb1accb0ca",
   "metadata": {},
   "source": [
    "### Cumulative Probability Distribution (CDF)\n",
    "\n",
    "Contrasting the discrete probability density whose random variable take on discrete values, the continuous density's random variable takes on continuous, and real values. \n",
    "\n",
    "Let $X$ be the continuous random variable, then the cumulative probability distribution function $F(x)$ is defined as:\n",
    "$$F(x) = P(X \\leq x)$$\n",
    "\n",
    "In other words, the probability that the random variable $X$ is less than or equal to $x\n",
    "\n",
    "An important property of a CDF is that:\n",
    "$$F(\\infty) = P(X \\leq \\infty) = 1$$\n",
    "\n",
    "### Probability Density Function (PDF)\n",
    "\n",
    "> By definition, a probability density function is:\n",
    "$$\\frac{dF}{dx} = f(x)$$\n",
    "and that $$\\int_{-\\infty}^{\\infty} f(x) dx = 1$$\n",
    "\n",
    "If we think that the range of the random variable $X$ is the sample space, then the sum of all events occuring must sum to 1 by $P(\\Omega) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c6048-2520-4813-b4e5-f8a267a259bd",
   "metadata": {},
   "source": [
    "<ins>Example 1: Normal Distribution</ins>\n",
    "\n",
    "Perhaps the most important continuous distribution used commonly to describe naturally occurring data is the normal distribution (also known as the Gaussian Distribution).\n",
    "\n",
    "> The Normal Distribution with mean $\\mu$ and standard deviation $\\sigma$; $N(\\mu, \\sigma)$ has probability density function:\n",
    "$$f(x;\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "In particular, a special case of the normal distribution with $\\mu=0$ and $\\sigma=1$ is know as the **standard normal**, whose distribution is $\\Phi(z)$.\n",
    "\n",
    "A useful fact is that any $X \\sim N(\\mu, \\sigma)$ can be converted into a standard normal and its probability can be looked up from standard normal tables\n",
    "\n",
    "Example:\n",
    "\n",
    "If $X \\sim N(2, 5)$, $P(X \\leq 4) = P\\left(\\sqrt{5}Z + 2 \\leq 4\\right) = P\\left( Z \\leq \\frac{4-2}{\\sqrt{5}}\\right) = \\Phi\\left(\\frac{4-2}{\\sqrt{5}}\\right)$\n",
    "\n",
    "This property will be important for applying the Central Limit Theorem\n",
    "\n",
    "<ins>Example 2: Exponential Distribution</ins>\n",
    "> The exponential distribution has a rate of $\\lambda$, with density function:\n",
    "$$f(x; \\lambda) = \\lambda \\exp{(-\\lambda x)}, x\\in [0, \\infty)$$\n",
    "With a CDF:\n",
    "$$F(x) = 1 - \\exp{(-\\lambda x)}$$\n",
    "$$S(x) \\equiv 1 - F(x) = \\exp{(-\\lambda x)}$$\n",
    "\n",
    "The survival function $S(x)$ is very important when discussing insurance claims, life expectancy, etc.\n",
    "\n",
    "<ins>Example 3: Gamma Distribution</ins>\n",
    "\n",
    "> The Gamma Distribution $X \\sim \\Gamma(\\alpha, \\beta)$ has density:\n",
    "$$f(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1} e^{-\\beta x} \\beta^\\alpha}{\\Gamma(\\alpha)}$$\n",
    "where $\\Gamma(\\alpha) = (\\alpha -1)!$\n",
    "\n",
    "The Exponential Distribution defined above is a specific example of a Gamma Distribution with $\\alpha=1, \\beta=\\lambda$\n",
    "\n",
    "An important property of the Gamma Distribution is that:\n",
    "$$\\Gamma(n, \\beta) = \\sum_{i=1}^n \\Gamma(1, \\beta)$$\n",
    "This is important in stochastic processes when we wish to find the $n$th arrival time of a Poisson process\n",
    "\n",
    "<ins>Example 4: Chi-Squared Distribution</ins>\n",
    "\n",
    "The Chi-Squared Distribution with $n$ degrees of freedom, $X \\sim \\chi^2(n)$, is another version of the Gamma Distribution with parameters $$\\chi^2(n) = \\Gamma\\left( \\frac{n}{2}, 2 \\right) = \\sum_{i=1}^n \\Gamma\\left( \\frac{1}{2}, 2 \\right)$$\n",
    "\n",
    "An important property of the Gamma distribution from what we observe here is that a sum of Gamma is a Gamma, provided that they have the same $\\beta$ parameter.\n",
    "\n",
    "> The $\\chi^2(n)$ distribution can also be written as a sum of squared standard normal random variable: \n",
    "$$\\chi^2(n) = \\sum_{i=1}^n Z_i^2$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
